{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Training (Model Type: `ana-cont`)\n",
    "\n",
    "Train a CNN network to extract the needle position of an analog dial based on **sin/cos coding and angle calculation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Parameter\n",
    "\n",
    "IMPORTANT: Do not rename any variables in this section â€” they are externally referenced in the GitHub action `Train Model`.\n",
    "\n",
    "* `TFlite_MainType`: Model type name\n",
    "* `TFlite_Version`: Model version identifier\n",
    "* `TFlite_Size`: Model architecture defined by size\n",
    "* `Input_Dir`: Input directory path containing training images\n",
    "* `Output_Dir`: Output directory path where results (models, logs, etc.) will be saved\n",
    "* `Input_Shape`: Image dimensions (width, height, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Model type (No need to adapt)\n",
    "TFlite_MainType: str = 'ana-cont'\n",
    "\n",
    "# Define model version (e.g. 1700 -> v17.00)\n",
    "TFlite_Version: str  = 'xxxx'\n",
    "\n",
    "# Choose model size\n",
    "TFlite_Size: str     = 's0'\n",
    "# Model size pool (Available models: `./src/models/ana_cont.py`)\n",
    "Available_Model_Sizes = {'s0', 's1', 's2', 's3'}\n",
    "\n",
    "# Input folder\n",
    "# Images will be resized automatically \n",
    "# Exception: Using folder name: `data_resize_all`\n",
    "Input_Dir: str  = 'data_raw_all'\n",
    "\n",
    "# Output folder\n",
    "Output_Dir: str = 'models/ana-cont'\n",
    "\n",
    "# Input image size [width, height, channels]\n",
    "Input_Shape = (32, 32, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.models.ana_cont import *\n",
    "from src.utils.augmentation import random_invert_image, random_white_balance\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.plot_functions import (plot_loss, plot_dataset_distribution, plot_dataset_analog, \n",
    "                                      plot_dataset_analog_result, plot_divergence)\n",
    "from src.models.evaluation import predict_and_evaluate, get_false_predictions\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "# Make sure version is at least 4 characters long if defined with digits only (e.g. papermill paramter 100 -> 0100)\n",
    "if str(TFlite_Version).isdigit() and len(str(TFlite_Version)) < 4:\n",
    "    TFlite_Version = str(TFlite_Version).zfill(4)\n",
    "\n",
    "\n",
    "# Validate model size input\n",
    "if TFlite_Size not in Available_Model_Sizes:\n",
    "    raise ValueError(f\"Invalid TFlite_Size '{TFlite_Size}'. Must be one of: {', '.join(Available_Model_Sizes)}\")\n",
    "\n",
    "\n",
    "# Prepare folders\n",
    "if not (Path(Input_Dir).exists() and Path(Input_Dir).is_dir()): # Check if input is availabe\n",
    "    sys.exit(f\"Folder '{Input_Dir}' does not exist.\")\n",
    "    \n",
    "Path(Output_Dir).mkdir(parents=True, exist_ok=True)  # Create output folder if it doesn't exist\n",
    "\n",
    "\n",
    "# Disable GPUs\n",
    "try:\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images\n",
    "\n",
    "* The images are expected in the \"Input_Dir\"\n",
    "* The image size must be 32 x 32 with 3 color channels (RGB)\n",
    "* The first 3 digits of image filename must contain the real value representation of the image:\n",
    "  * Generic: `x.y_zzzz.jpg`\n",
    "  * Example: `4.6_main_ana1_2019-06-02T050011.jpg`\n",
    "\n",
    "| Filename Part | Description                  | Usage                    |\n",
    "|---------------|------------------------------|--------------------------|\n",
    "| x.y           | Represented value (e.g. 4.6) | **Value to be learned**  |\n",
    "| _zzzz         | Further file description     | Not required / processed |\n",
    "\n",
    "* The images are stored in the x_data[]\n",
    "* The expected output for each image in the corresponding y_data[]\n",
    "  * The periodic nature is reflected in a **sin/cos coding**, which allows to restore the angle/counter value with an arctan later on.\n",
    "* The last step is a shuffle (from sklearn.utils) as the filenames are on order due to the encoding of the expected analog readout in the filename\n",
    "* Split dataset into training and validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f\"{Input_Dir}/*.jpg\")\n",
    "num_files = len(files)\n",
    "\n",
    "# Prepare data containers\n",
    "f_data = np.empty(num_files, dtype=\"<U250\")\n",
    "x_data = np.empty((num_files, Input_Shape[0], Input_Shape[1], Input_Shape[2]), dtype=\"float32\")\n",
    "y_data = np.empty((num_files, 2), dtype=\"float32\")\n",
    "y_data_target = np.empty(num_files)\n",
    "\n",
    "# Process files\n",
    "for i, file in enumerate(files):\n",
    "    # Read image file\n",
    "    image_bytes = tf.io.read_file(file)\n",
    "    image = tf.image.decode_image(image_bytes, channels=Input_Shape[2], expand_animations=False)\n",
    "\n",
    "    # Resize image (if required)\n",
    "    if image.shape[0] != Input_Shape[0] or image.shape[1] != Input_Shape[1]:\n",
    "        image = tf.image.resize(image, [Input_Shape[0], Input_Shape[1]], method=tf.image.ResizeMethod.MITCHELLCUBIC)\n",
    "        image = tf.clip_by_value(tf.cast(image, tf.float32), 0.0, 255.0)\n",
    "    else:\n",
    "        image = tf.cast(image, tf.float32)\n",
    "\n",
    "    # Extract truth value from filename and calculate respective sin/cos values\n",
    "    base = Path(file).name\n",
    "    target_number = float(base[:3]) / 10\n",
    "    target_sin = math.sin(target_number * math.pi * 2)\n",
    "    target_cos = math.cos(target_number * math.pi * 2)\n",
    "\n",
    "    # Save data\n",
    "    f_data[i] = file\n",
    "    x_data[i] = image.numpy()\n",
    "    y_data[i] = [target_sin, target_cos]\n",
    "    y_data_target[i] = target_number\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(f\"{i} files processed...\")\n",
    "\n",
    "\n",
    "print(\"Data count: \", len(y_data))\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Distribution\n",
    "\n",
    "Uneven distribution of data can lead to poorer results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_dataset_distribution(y_data_target)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "**Important**\n",
    "* Shape of the input layer: (32, 32, 3)\n",
    "* Shape of the output layer: (2) - sin and cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (TFlite_Size == \"s0\"):\n",
    "    model = model_ana_cont_s0(input_shape=(Input_Shape[0], Input_Shape[1], Input_Shape[2]), learning_rate=1e-3)\n",
    "elif (TFlite_Size == \"s1\"):\n",
    "    model = model_ana_cont_s1(input_shape=(Input_Shape[0], Input_Shape[1], Input_Shape[2]), learning_rate=1e-3)\n",
    "elif (TFlite_Size == \"s2\"):\n",
    "    model = model_ana_cont_s2(input_shape=(Input_Shape[0], Input_Shape[1], Input_Shape[2]), learning_rate=1e-3)\n",
    "elif (TFlite_Size == \"s3\"):\n",
    "    model = model_ana_cont_s3(input_shape=(Input_Shape[0], Input_Shape[1], Input_Shape[2]), learning_rate=1e-3)\n",
    "else:\n",
    "    raise ValueError(f\"TFlite_Size: '{TFlite_Size}' is not supported.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Datasets / Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "###############################\n",
    "Validation_Percentage = 0.2 # (0.0 == 0%: Use all images for training, none for validation)\n",
    "Batch_Size = 8\n",
    "Shift_Range = 1\n",
    "Brightness_Range = 0.2\n",
    "Zoom_Range = 0.05\n",
    "Channel_Shift_Range = 5\n",
    "Shear_Range = 1\n",
    "###############################\n",
    "\n",
    "\n",
    "# Augmentation helper function\n",
    "def preprocessing(x):\n",
    "    x = random_white_balance(x)\n",
    "    x = random_invert_image(x)\n",
    "    x = np.clip(x, 0.0, 255.0)\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "\n",
    "# Shuffle dataset\n",
    "x_data, y_data, y_data_target, f_data = shuffle(x_data, y_data, y_data_target, f_data)\n",
    "\n",
    "# Split data into training and validation dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=Validation_Percentage)\n",
    "\n",
    "# Prepare training dataset (with augmentation)\n",
    "print(\"Training data\")\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=[-Shift_Range, Shift_Range], \n",
    "    height_shift_range=[-Shift_Range, Shift_Range],\n",
    "    brightness_range=[1 - Brightness_Range, 1 + Brightness_Range],\n",
    "    zoom_range=[1 - Zoom_Range, 1 + Zoom_Range],\n",
    "    channel_shift_range=Channel_Shift_Range,\n",
    "    shear_range=Shear_Range,\n",
    "    preprocessing_function=preprocessing\n",
    ")\n",
    "\n",
    "train_iterator = datagen.flow(x_train, y_train, batch_size=Batch_Size)\n",
    "plot_dataset_analog(train_iterator)\n",
    "\n",
    "# Prepare validation dataset (without augmentation)\n",
    "if (Validation_Percentage > 0):\n",
    "    datagen_val = ImageDataGenerator() # No augmentation for validation\n",
    "    validation_iterator = datagen_val.flow(x_test, y_test, batch_size=Batch_Size)\n",
    "    print(\"  \")\n",
    "    print(\"Validation data\")\n",
    "    plot_dataset_analog(validation_iterator, rows=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    " \n",
    "* Train the model using augmented training data and unaltered validation data\n",
    "* Visualize training and validation performance over epochs (e.g., loss and accuracy curves)\n",
    "* Avoid significant overfitting: The validation performance should not deviate heavily from training due to augmentation being applied only to the training data\n",
    "* Best-performing model automatically selected, not necessarily the final one. The optimal model is often found within the last ~30-40 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameter\n",
    "###############################\n",
    "Training_Epochs = 600\n",
    "###############################\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.9, patience=5, min_lr=1e-5, verbose=0\n",
    ")\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', mode='min', patience=40, restore_best_weights=True, verbose=0\n",
    ")\n",
    "\n",
    "# Train model\n",
    "if (Validation_Percentage > 0):\n",
    "    history = model.fit(\n",
    "        train_iterator, validation_data = validation_iterator, epochs = Training_Epochs, \n",
    "        callbacks=[lr_scheduler, early_stopping], verbose=0\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        train_iterator, epochs = Training_Epochs, \n",
    "        callbacks=[lr_scheduler, early_stopping], verbose=0\n",
    "    )\n",
    "\n",
    "# Plot model loss graph\n",
    "_ = plot_loss(history, validation=Validation_Percentage > 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model verification\n",
    "\n",
    "* The following code uses the trained model to check the deviation for each picture (train + validation).\n",
    "* The accepted_deviation can be used to get the accuracy with allowed differences (for instance +/- 0.1)\n",
    "* The first (max) 49 false predicted images will be shown\n",
    "* A csv-file with all false predicted images will be created. It can be used for relabeling with this tool: https://github.com/haverland/collectmeteranalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "###############################\n",
    "Accepted_Deviation = 0.1\n",
    "###############################\n",
    "\n",
    "# Predict and evaluate results\n",
    "predicted_val, expected_val, deviation_val = predict_and_evaluate(model=model, x_data=x_data, y_data=y_data, y_data_target=y_data_target)\n",
    "\n",
    "# Process false predictions\n",
    "false_predictions_result = get_false_predictions(expected_val=expected_val, predicted_val=predicted_val, deviation_val=deviation_val, \n",
    "                               x_data=x_data, f_data=f_data, accepted_deviation=Accepted_Deviation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot false predicted divergation\n",
    "print(f\"Accuracy: {(1 - len(false_predictions_result['dev']) / len(y_data)) * 100.0:.2f} % (Images: {len(y_data)} | False Predicted: {len(false_predictions_result['dev'])}) | Accepted Deviation: {Accepted_Deviation}\")\n",
    "\n",
    "title = f\"False Predicted Divergation  |  Images: {len(y_data)}\\nAccuracy: {(1 - len(false_predictions_result['dev']) / len(y_data)) * 100.0:.2f} % (False Predicted: {len(false_predictions_result['dev'])}) With Accepted Deviation: {Accepted_Deviation}\"\n",
    "_ = plot_divergence(np.bincount(np.array(np.round((np.abs(false_predictions_result['dev']) * 10) % 51).astype(int)), minlength=51), title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset of false predictions (Use first 49 entries)\n",
    "print(\"False Predictions (Sorted by highest deviation, max. 49 images)\")\n",
    "plot_dataset_analog_result(false_predictions_result[\"img\"], false_predictions_result[\"lbl\"], columns=7, rows=7, figsize=(18,18))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save false predicted image list to CSV\n",
    "# The csv file can be further processed with the tool [collectmeteranalog](https://github.com/haverland/collectmeteranalog) to evaluate or adjust labels\n",
    "\n",
    "csv_dir = f\"{Output_Dir}/training_details/{TFlite_MainType}_{TFlite_Version}_{TFlite_Size}/\"\n",
    "Path(csv_dir).mkdir(parents=True, exist_ok=True)  # Create csv folder if it doesn't exist\n",
    "\n",
    "false_predicted_df = pd.DataFrame(list(zip(false_predictions_result[\"file\"], false_predictions_result[\"pred\"], false_predictions_result[\"exp\"], \n",
    "                                  false_predictions_result[\"dev\"])), columns=[\"File\", \"Predicted\", \"Expected\", \"Deviation\"])\n",
    "false_predicted_df.to_csv(f\"{csv_dir}/{TFlite_MainType}_{TFlite_Version}_{TFlite_Size}_false_predicted.csv\", index=True, index_label=\"Index\")\n",
    "\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "# Save false predicted image list to CSV (--> LEGACY File Syntax)\n",
    "# The csv file can be further processed with the tool [collectmeteranalog](https://github.com/haverland/collectmeteranalog) to evaluate or adjust labels\n",
    "\n",
    "#false_predicted_df = pd.DataFrame(false_predicted_files)\n",
    "#false_predicted_df.to_csv(f\"{csv_dir}/{TFlite_MainType}_{TFlite_Version}_{TFlite_Size}_false_predicted.csv\", index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "* Save the model to the file with the \"tflite\" file format\n",
    "* quantize the model and store it as _q.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileName = f\"{Output_Dir}/{TFlite_MainType}_{TFlite_Version}_{TFlite_Size}.tflite\"\n",
    "\n",
    "# TensorFlow Lite conversion\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open(FileName, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"Model saved successfully. File: {FileName}\")\n",
    "print(f\"File size: {Path(FileName).stat().st_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileName = f\"{Output_Dir}/{TFlite_MainType}_{TFlite_Version}_{TFlite_Size}_q.tflite\"\n",
    "\n",
    "# Representative dataset function\n",
    "def representative_dataset():\n",
    "    for n in range(x_data.shape[0]):\n",
    "        data = np.expand_dims(x_data[n], axis=0)\n",
    "        yield [data.astype(np.float32)]\n",
    "\n",
    "\n",
    "# TensorFlow Lite conversion with optimizations\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "converter._experimental_disable_per_channel_quantization_for_dense_layers = True\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the converted model to the specified file\n",
    "with open(FileName, \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(f\"Model saved successfully. File: {FileName}\")\n",
    "print(f\"File size: {Path(FileName).stat().st_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
